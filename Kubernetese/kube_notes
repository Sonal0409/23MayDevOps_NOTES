
Command to check number of nodes in cluster

$ kubectl get nodes


Schedule a pod in kube cluster

$ kubectl run pod1 --image nginx

similar to docker

 $ docker run --name c1 nginx

Check the pods is created or not in the cluster

 $ kubectl get pods

which Node is the pods scehduled:

$ kubectl get pods -o wide

similar to docker

docker ps -a

How did all this happen

to see all the details of the container:

$ kubectl describe pod pod1 | less

==> press q to come out of it 

similar to docker

docker inspect <containername>


Create a pod using object definition file:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2

Multi container POD
***************************
apiVersion: v1
kind: Pod
metadata:
  name: multi-container
spec:
  containers:
  - name: nginx
    image: nginx:1.10-alpine
  - name: alpine
    image: alpine:3.5
    command: ["watch", "wget", "-qO-", "localhost"]


$ kubectl create -f pod-definition.yml



*************************

ReplicaSet

---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
 name: myrs
 labels:
  type: webserver
spec:
 replicas: 3
 selector:
  matchLabels:
   type: webserver
 template:
  metadata:
   name: mypod
   labels:
    type: webserver
  spec:
   containers:
    - name: myn1
      image: nginx
      
      
      
      Create replicaset
      
      $ kubectl create -f <name of the file.yml>
      
      Get all the object created in cluster
      
      $ kubectl get all
      
      get pods created in the cluster based on the labels
      
      $ kubectl get pods -l type=webserver
      
      Describe the details of the replicaSet
      
      $ kubectl describe replicaset myrs | less
      
      
      Deployment:
      *********************
      
      
      kind: Deployment
apiVersion: apps/v1
metadata:
  name: kubeserve
spec:
  replicas: 3
  minReadySeconds: 10 # wait for 45 sec before going to deploy next pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1  
      maxSurge: 1        # max number of pods to run for the deployment
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      containers:
       - name: app
         image: leaddevops/kubeserve:v1
         
    $ kubectl create -f deployment1.yml
    
    $ kubectl get all
    
    $ kubectl set image deployment kubeserve app=leaddevops/kubeserve:v2
    
    $ kubectl describe deployment kubeserve | less
    
    $ kubectl set image deployment kubeserve app=leaddevops/kubeserve:v3
    
    $ $ kubectl describe deployment kubeserve | less
    
  565  kubectl rollout history deployment kubeserve
  566  kubectl rollout undo deployment kubeserve --to-revision=1
  567  kubectl describe deployment kubeserve | less


Horizontal Pod AutoScaller:

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginxpod
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          resources:
            limits:
              cpu: 10m

$ kubectl create -f deployment2.yml

---

apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  type: ClusterIP  ## this is default if we do not type in service definition
  selector:
    app: nginx
  ports:
   - protocol: TCP
     port: 80
     targetPort: 80

$ kubectl create -f service2.yml
---

apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 5
  
  $ kubectl create -f hpa.yml
  $ kubectl get all
  
  Load Genrator command:
  kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://10.8.0.237; done"
  
  
  ***************************************************************
  
  Persistent Volume:
  ************************
  apiVersion: v1
kind: PersistentVolume
metadata:
 name: block-pv
spec:
 storageClassName: manual
 capacity: 
  storage: 1Gi
 accessModes:
  - ReadWriteOnce
 hostPath:
  path: /tmp/data
  
  ***********************
  $ kubectl get pv
  $ kubectl create -f pv.yml
  
  *****************
  vim pvc.yml
  
  apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: pvc
spec:
 storageClassName: manual
 accessModes:
  - ReadWriteOnce
 resources:
  requests:
   storage: 1Gi
   
   $ kubectl get pvc
   $ kubectl create -f pvc.yml
   
   **************
   
   vim pod-pvc.yml
   
   apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: pvc
spec:
 storageClassName: manual
 accessModes:
  - ReadWriteOnce
 resources:
  requests:
   storage: 1Gi
   
   ********
   $ kubectl get pods
   kubectl create -f pod-pvc.yml
  
  
  
  # kubectl get configmap
 
 # kubectl create configmap dev-config --from-literal=app.mem=2048m
 
 # kubectl get configmap
 
 # kubectl get configmap dev-config -o yaml
 # vim dev.properties
app.env:dev
app.mem=2048m
app.properties=dev.env.url
:wq!

# kubectl create configmap dev-config1 --from-file=dev.properties
# kubectl get configmap
# kubectl get configmap dev-config1 -o yaml

Use configmap for a pod

vim pod-configmap.yml

kind: Pod
apiVersion: v1
metadata:
 name: pod-configmap
spec:
 containers:
  - image: nginx
    name: c1
    volumeMounts:
     - name: config-volume
       mountPath: /etc/config
 volumes:
  - name: config-volume
    configMap:
     name: dev-config1
 restartPolicy: Never
 
 :wq!
 
 # kubectl apply -f pod-configmap.yml
 # kubectl exec -it pod-configmap bash
 # cd /etc/config
 
 you will find the dev.properties file and configurations
 
 Edit the configMAP
 
 kubectl edit configmap -n <namespace> <configMapName> -o yaml

This opens up a vim editor with the configmap in yaml format. Now simply edit it and save it.

**************************************

Namespaces
In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc).

When to Use Multiple Namespaces
Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.

Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.

Namespaces are a way to divide cluster resources between multiple users (via resource quota).

It is not necessary to use multiple namespaces to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace
